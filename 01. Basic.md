
## Nueral network
- 퍼셉트론, 다층 퍼셉트론(MLP, multilayer perceptron)
- Activation function, Gradient Descent, SGD
- Back propagation


## Sequential 
- 가장 간단한 모델
- 선형 파이프라인(스택)을 정의

#### Dense ?
- 각 레이어의 뉴런들이, 인접한 레이어의 모든 뉴런들과 빽빽히 연결되어 있는 구조


```python
from keras.layers.core import Dense
from keras.models import Sequential

# model = Sequential()
# model.add(Dense(12, input_dim=8, kernel_initializer="random_uniform"))
```

    Using TensorFlow backend.


#### Weight 초기화 옵션들 ( https://keras.io/initializers/ )
- random_uniform, random_normal, zero 등등..
- 참고자료 ; uniform distribution vs. normal distribution  
   (https://www.quora.com/What-is-the-difference-between-normal-distribution-and-uniform-distribution)

#### Activation function ( https://keras.io/activations/ )
- sigmoid ; f(x) = 1 / (1 + e^(-x))
- ReLU ; f(x) = max(0, x)
- etc.

## MNIST 예제로 감잡기
- 6만개 학습 셋, 1만개 테스트 셋
- 각 클래스는 One hot encoding으로 표현


```python
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.optimizers import SGD, Adam
from keras.utils import np_utils

import matplotlib.pyplot as plt

NB_EPOCH = 20
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10
OPTIMIZER = Adam()
N_HIDDEN = 128
VALIDATION_SPLIT = 0.2 # training data 중 얼마나 validation set으로 쓸지
DROPOUT = 0.3
```

#### 데이터 로드


```python
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```


```python
X_train.shape, y_train.shape, X_test.shape, y_test.shape
```




    ((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))




```python
RESHAPED = 784

X_train = X_train.reshape(60000, RESHAPED).astype('float32')
X_test = X_test.reshape(10000, RESHAPED).astype('float32')

# 0~1
X_train /= 255
X_test /= 255

# one hot vector
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)
```

#### 모델 정의


```python
model = Sequential()
model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(N_HIDDEN))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_1 (Dense)              (None, 128)               100480    
    _________________________________________________________________
    activation_1 (Activation)    (None, 128)               0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 128)               16512     
    _________________________________________________________________
    activation_2 (Activation)    (None, 128)               0         
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 10)                1290      
    _________________________________________________________________
    activation_3 (Activation)    (None, 10)                0         
    =================================================================
    Total params: 118,282
    Trainable params: 118,282
    Non-trainable params: 0
    _________________________________________________________________


#### Loss Function 
- https://keras.io/losses/
- MSE ; 예측 값, 실제 값 사이의 평균 제곱 오차
- Binary Cross Entropy ; *-t* x *log(p)* - *(1 - t)* x *log(1 - p)*
- etc.

#### Metric
- https://keras.io/metrics
- Accuracy, Precisionm, Recall

#### Optimizer
- https://keras.io/optimizers
- SGD, RMSprop, Adam (2개의 후자가 더 빨리 수렴. SGD에선 200에폭에 달성한 것을 20에폭에 달성할 수)

#### 모델 컴파일


```python
model.compile(loss='categorical_crossentropy',
             optimizer=OPTIMIZER,
             metrics=['accuracy'])
```

#### 모델 학습
- epochs ; 학습 데이터셋 전체를 살펴본 횟수
- batch_size ; 옵티마이저가 가중치 업데이트를 하기 전까지 살펴본 데이터 수
- **학습 정확도는 반드시 테스트 정확도보다 높아**야 함!! (그렇지 않으면 충분히 학습된게 아니라하네)


```python
history = model.fit(X_train, Y_train,
                   batch_size=BATCH_SIZE, epochs=NB_EPOCH,
                   verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
```

    Train on 48000 samples, validate on 12000 samples
    Epoch 1/20
    48000/48000 [==============================] - 1s 30us/step - loss: 0.5157 - acc: 0.8420 - val_loss: 0.1846 - val_acc: 0.9465
    Epoch 2/20
    48000/48000 [==============================] - 1s 28us/step - loss: 0.2315 - acc: 0.9322 - val_loss: 0.1395 - val_acc: 0.9582
    Epoch 3/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.1751 - acc: 0.9475 - val_loss: 0.1090 - val_acc: 0.9674
    Epoch 4/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.1482 - acc: 0.9558 - val_loss: 0.1003 - val_acc: 0.9697
    Epoch 5/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.1264 - acc: 0.9610 - val_loss: 0.0948 - val_acc: 0.9712
    Epoch 6/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.1140 - acc: 0.9649 - val_loss: 0.0858 - val_acc: 0.9739
    Epoch 7/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.1062 - acc: 0.9680 - val_loss: 0.0859 - val_acc: 0.9752
    Epoch 8/20
    48000/48000 [==============================] - 1s 28us/step - loss: 0.0948 - acc: 0.9709 - val_loss: 0.0847 - val_acc: 0.9755
    Epoch 9/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0854 - acc: 0.9727 - val_loss: 0.0772 - val_acc: 0.9778
    Epoch 10/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.0843 - acc: 0.9732 - val_loss: 0.0807 - val_acc: 0.9767
    Epoch 11/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0766 - acc: 0.9759 - val_loss: 0.0766 - val_acc: 0.9781
    Epoch 12/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0746 - acc: 0.9755 - val_loss: 0.0782 - val_acc: 0.9782
    Epoch 13/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0705 - acc: 0.9776 - val_loss: 0.0733 - val_acc: 0.9771
    Epoch 14/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0695 - acc: 0.9780 - val_loss: 0.0808 - val_acc: 0.9762
    Epoch 15/20
    48000/48000 [==============================] - 1s 25us/step - loss: 0.0621 - acc: 0.9793 - val_loss: 0.0763 - val_acc: 0.9788
    Epoch 16/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.0625 - acc: 0.9798 - val_loss: 0.0769 - val_acc: 0.9789
    Epoch 17/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.0612 - acc: 0.9809 - val_loss: 0.0736 - val_acc: 0.9791
    Epoch 18/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.0560 - acc: 0.9821 - val_loss: 0.0752 - val_acc: 0.9782
    Epoch 19/20
    48000/48000 [==============================] - 1s 27us/step - loss: 0.0552 - acc: 0.9823 - val_loss: 0.0765 - val_acc: 0.9797
    Epoch 20/20
    48000/48000 [==============================] - 1s 26us/step - loss: 0.0516 - acc: 0.9831 - val_loss: 0.0790 - val_acc: 0.9798


#### 모델 평가


```python
score = model.evaluate(X_test, Y_test, verbose=VERBOSE)
print("Test score:", score[0])
print("Test accuracy:", score[1])
```

    10000/10000 [==============================] - 0s 25us/step
    Test score: 0.080255606818042
    Test accuracy: 0.9778


#### 시각화


```python
history.history.keys()
```




    dict_keys(['val_acc', 'val_loss', 'acc', 'loss'])




```python
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```


![png](images/01.%20Basic_22_0.png)



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```


![png](images/01.%20Basic_23_0.png)


#### 기타
- 뭐든 **변경한 이후에 직접 측정**해봐야 알겠지만
- 위 예제에선 LR = 0.001 이 적당
- Dropout도 0.3이 적당 했음.
- SGDs는 Batch size 만큼의 데이터만을 고려해 학습하는 방법인데, 
    - Batch 크기에 따라서도 정확도가 달라 질 수 있다. 64, 128, 256, ... 다 해보기

#### 참고사항
- 모델이 너무 복잡하면 학습 데이터의 수많은 내재적 관계까지 학습해 버릴 가능성 있음
- 이렇게되면 처음 본 데이터에 대해 일반화 할 수 없음.
- Overfitting
- 보통 validation set에 대한 loss는 감소하다가 증가하는 경향 있다. 그 시점에 오버피팅을 의심해 봐야함
- 모델 A, B가 있고 A가 B보다 복잡할 때 두 개가 거의 동일한 성능을 보인다면 덜 복잡한 B를 선택하는게 낫다

#### Regularization이 필요하다!
- https://keras.io/regularizers
- L1 regularization(lasso) ; 모델의 복잡도를 가중치 절댓값의 합으로 표현
- L2 regularization(ridge) ; 모델의 복잡도를 가중치의 제곱 합으로 표현
- Elastic net regularization ; 앞에 두가지 조합

#### 예측하기
- model.predict()
- model.evaluate() ; loss 값 계산
- model.predict_classes() ; 범주 출력 계산
- model.predict_proba() ; 범주 확률 계산


```python
predictions = model.predict_classes(X_test[0].reshape(-1, 784))
```


```python
predictions, Y_test[0]
```




    (array([7]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]))


