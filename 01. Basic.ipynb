{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral network\n",
    "- 퍼셉트론, 다층 퍼셉트론(MLP, multilayer perceptron)\n",
    "- Activation function, Gradient Descent, SGD\n",
    "- Back propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential \n",
    "- 가장 간단한 모델\n",
    "- 선형 파이프라인(스택)을 정의\n",
    "\n",
    "#### Dense ?\n",
    "- 각 레이어의 뉴런들이, 인접한 레이어의 모든 뉴런들과 빽빽히 연결되어 있는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(12, input_dim=8, kernel_initializer=\"random_uniform\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight 초기화 옵션들 ( https://keras.io/initializers/ )\n",
    "- random_uniform, random_normal, zero 등등..\n",
    "- 참고자료 ; uniform distribution vs. normal distribution  \n",
    "   (https://www.quora.com/What-is-the-difference-between-normal-distribution-and-uniform-distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function ( https://keras.io/activations/ )\n",
    "- sigmoid ; f(x) = 1 / (1 + e^(-x))\n",
    "- ReLU ; f(x) = max(0, x)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 예제로 감잡기\n",
    "- 6만개 학습 셋, 1만개 테스트 셋\n",
    "- 각 클래스는 One hot encoding으로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # training data 중 얼마나 validation set으로 쓸지\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED).astype('float32')\n",
    "X_test = X_test.reshape(10000, RESHAPED).astype('float32')\n",
    "\n",
    "# 0~1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot vector\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function \n",
    "- https://keras.io/losses/\n",
    "- MSE ; 예측 값, 실제 값 사이의 평균 제곱 오차\n",
    "- Binary Cross Entropy ; *-t* x *log(p)* - *(1 - t)* x *log(1 - p)*\n",
    "- etc.\n",
    "\n",
    "#### Metric\n",
    "- https://keras.io/metrics\n",
    "- Accuracy, Precisionm, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=OPTIMIZER,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습\n",
    "- epochs ; 학습 데이터셋 전체를 살펴본 횟수\n",
    "- batch_size ; 옵티마이저가 가중치 업데이트를 하기 전까지 살펴본 데이터 수\n",
    "- **학습 정확도는 반드시 테스트 정확도보다 높아**야 함!! (그렇지 않으면 충분히 학습된게 아니라하네)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 1.7526 - acc: 0.4450 - val_loss: 0.9427 - val_acc: 0.8023\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.9411 - acc: 0.7125 - val_loss: 0.5453 - val_acc: 0.8633\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.7101 - acc: 0.7834 - val_loss: 0.4324 - val_acc: 0.8833\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.6015 - acc: 0.8168 - val_loss: 0.3777 - val_acc: 0.8940\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.5430 - acc: 0.8362 - val_loss: 0.3435 - val_acc: 0.9031\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.4979 - acc: 0.8520 - val_loss: 0.3194 - val_acc: 0.9080\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4630 - acc: 0.8629 - val_loss: 0.3024 - val_acc: 0.9110\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4395 - acc: 0.8707 - val_loss: 0.2866 - val_acc: 0.9152\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.4154 - acc: 0.8781 - val_loss: 0.2730 - val_acc: 0.9193\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.4003 - acc: 0.8805 - val_loss: 0.2618 - val_acc: 0.9237\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3823 - acc: 0.8886 - val_loss: 0.2532 - val_acc: 0.9248\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3678 - acc: 0.8927 - val_loss: 0.2442 - val_acc: 0.9273\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3548 - acc: 0.8956 - val_loss: 0.2352 - val_acc: 0.9301\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3452 - acc: 0.8987 - val_loss: 0.2281 - val_acc: 0.9325\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3346 - acc: 0.9023 - val_loss: 0.2211 - val_acc: 0.9353\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3214 - acc: 0.9051 - val_loss: 0.2143 - val_acc: 0.9375\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3140 - acc: 0.9080 - val_loss: 0.2088 - val_acc: 0.9394\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.3081 - acc: 0.9104 - val_loss: 0.2038 - val_acc: 0.9409\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2956 - acc: 0.9135 - val_loss: 0.1986 - val_acc: 0.9417\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2905 - acc: 0.9146 - val_loss: 0.1933 - val_acc: 0.9436\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2835 - acc: 0.9173 - val_loss: 0.1895 - val_acc: 0.9443\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2776 - acc: 0.9188 - val_loss: 0.1854 - val_acc: 0.9455\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2733 - acc: 0.9209 - val_loss: 0.1810 - val_acc: 0.9469\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2655 - acc: 0.9223 - val_loss: 0.1764 - val_acc: 0.9483\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2617 - acc: 0.9226 - val_loss: 0.1742 - val_acc: 0.9493\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2548 - acc: 0.9256 - val_loss: 0.1704 - val_acc: 0.9503\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2500 - acc: 0.9265 - val_loss: 0.1688 - val_acc: 0.9505\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2469 - acc: 0.9268 - val_loss: 0.1644 - val_acc: 0.9524\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2419 - acc: 0.9286 - val_loss: 0.1617 - val_acc: 0.9532\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2388 - acc: 0.9298 - val_loss: 0.1579 - val_acc: 0.9548\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2333 - acc: 0.9303 - val_loss: 0.1556 - val_acc: 0.9556\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2298 - acc: 0.9333 - val_loss: 0.1534 - val_acc: 0.9561\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2254 - acc: 0.9345 - val_loss: 0.1501 - val_acc: 0.9574\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2234 - acc: 0.9354 - val_loss: 0.1487 - val_acc: 0.9575\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2206 - acc: 0.9355 - val_loss: 0.1484 - val_acc: 0.9568\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2173 - acc: 0.9357 - val_loss: 0.1444 - val_acc: 0.9583\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2134 - acc: 0.9377 - val_loss: 0.1430 - val_acc: 0.9587\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2114 - acc: 0.9386 - val_loss: 0.1405 - val_acc: 0.9594\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2046 - acc: 0.9401 - val_loss: 0.1396 - val_acc: 0.9597\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2054 - acc: 0.9396 - val_loss: 0.1372 - val_acc: 0.9593\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2018 - acc: 0.9411 - val_loss: 0.1364 - val_acc: 0.9597\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9424 - val_loss: 0.1347 - val_acc: 0.9609\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9433 - val_loss: 0.1327 - val_acc: 0.9613\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1910 - acc: 0.9450 - val_loss: 0.1313 - val_acc: 0.9613\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1954 - acc: 0.9422 - val_loss: 0.1298 - val_acc: 0.9623\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1876 - acc: 0.9450 - val_loss: 0.1285 - val_acc: 0.9628\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1868 - acc: 0.9456 - val_loss: 0.1271 - val_acc: 0.9635\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1856 - acc: 0.9451 - val_loss: 0.1259 - val_acc: 0.9637\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1823 - acc: 0.9462 - val_loss: 0.1243 - val_acc: 0.9638\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1788 - acc: 0.9475 - val_loss: 0.1236 - val_acc: 0.9639\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1781 - acc: 0.9475 - val_loss: 0.1230 - val_acc: 0.9638\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1776 - acc: 0.9487 - val_loss: 0.1215 - val_acc: 0.9643\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1766 - acc: 0.9474 - val_loss: 0.1199 - val_acc: 0.9646\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1741 - acc: 0.9495 - val_loss: 0.1198 - val_acc: 0.9651\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1713 - acc: 0.9496 - val_loss: 0.1183 - val_acc: 0.9658\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1691 - acc: 0.9509 - val_loss: 0.1173 - val_acc: 0.9658\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1656 - acc: 0.9515 - val_loss: 0.1169 - val_acc: 0.9664\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1670 - acc: 0.9514 - val_loss: 0.1162 - val_acc: 0.9661\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1638 - acc: 0.9511 - val_loss: 0.1146 - val_acc: 0.9668\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1616 - acc: 0.9521 - val_loss: 0.1138 - val_acc: 0.9676\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1613 - acc: 0.9531 - val_loss: 0.1130 - val_acc: 0.9674\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1584 - acc: 0.9529 - val_loss: 0.1123 - val_acc: 0.9678\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1564 - acc: 0.9541 - val_loss: 0.1113 - val_acc: 0.9677\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1550 - acc: 0.9550 - val_loss: 0.1103 - val_acc: 0.9684\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1557 - acc: 0.9537 - val_loss: 0.1092 - val_acc: 0.9683\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1546 - acc: 0.9543 - val_loss: 0.1085 - val_acc: 0.9681\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1529 - acc: 0.9549 - val_loss: 0.1087 - val_acc: 0.9681\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1503 - acc: 0.9550 - val_loss: 0.1085 - val_acc: 0.9677\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1493 - acc: 0.9558 - val_loss: 0.1074 - val_acc: 0.9688\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1481 - acc: 0.9564 - val_loss: 0.1065 - val_acc: 0.9685\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1484 - acc: 0.9557 - val_loss: 0.1058 - val_acc: 0.9691\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1458 - acc: 0.9569 - val_loss: 0.1049 - val_acc: 0.9694\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1465 - acc: 0.9558 - val_loss: 0.1041 - val_acc: 0.9694\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1433 - acc: 0.9575 - val_loss: 0.1045 - val_acc: 0.9693\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1431 - acc: 0.9576 - val_loss: 0.1033 - val_acc: 0.9696\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1406 - acc: 0.9574 - val_loss: 0.1030 - val_acc: 0.9692\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1416 - acc: 0.9592 - val_loss: 0.1020 - val_acc: 0.9702\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1400 - acc: 0.9583 - val_loss: 0.1015 - val_acc: 0.9702\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1363 - acc: 0.9607 - val_loss: 0.1008 - val_acc: 0.9699\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1365 - acc: 0.9607 - val_loss: 0.1004 - val_acc: 0.9703\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1350 - acc: 0.9602 - val_loss: 0.1000 - val_acc: 0.9703\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1351 - acc: 0.9595 - val_loss: 0.0998 - val_acc: 0.9708\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1316 - acc: 0.9612 - val_loss: 0.0993 - val_acc: 0.9709\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1312 - acc: 0.9609 - val_loss: 0.0991 - val_acc: 0.9706\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1311 - acc: 0.9613 - val_loss: 0.0981 - val_acc: 0.9708\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1289 - acc: 0.9616 - val_loss: 0.0972 - val_acc: 0.9713\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1314 - acc: 0.9613 - val_loss: 0.0972 - val_acc: 0.9713\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1279 - acc: 0.9620 - val_loss: 0.0973 - val_acc: 0.9713\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1252 - acc: 0.9630 - val_loss: 0.0966 - val_acc: 0.9713\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1271 - acc: 0.9615 - val_loss: 0.0961 - val_acc: 0.9713\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1271 - acc: 0.9613 - val_loss: 0.0964 - val_acc: 0.9715\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1239 - acc: 0.9636 - val_loss: 0.0956 - val_acc: 0.9717\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1257 - acc: 0.9619 - val_loss: 0.0954 - val_acc: 0.9718\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1242 - acc: 0.9630 - val_loss: 0.0952 - val_acc: 0.9721\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1228 - acc: 0.9635 - val_loss: 0.0952 - val_acc: 0.9722\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1227 - acc: 0.9629 - val_loss: 0.0939 - val_acc: 0.9724\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1193 - acc: 0.9646 - val_loss: 0.0934 - val_acc: 0.9728\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1201 - acc: 0.9636 - val_loss: 0.0932 - val_acc: 0.9726\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1167 - acc: 0.9648 - val_loss: 0.0934 - val_acc: 0.9723\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1181 - acc: 0.9645 - val_loss: 0.0930 - val_acc: 0.9722\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1171 - acc: 0.9652 - val_loss: 0.0925 - val_acc: 0.9723\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1161 - acc: 0.9654 - val_loss: 0.0926 - val_acc: 0.9727\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1149 - acc: 0.9664 - val_loss: 0.0917 - val_acc: 0.9722\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1162 - acc: 0.9653 - val_loss: 0.0913 - val_acc: 0.9732\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1138 - acc: 0.9663 - val_loss: 0.0910 - val_acc: 0.9728\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1129 - acc: 0.9667 - val_loss: 0.0908 - val_acc: 0.9729\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1146 - acc: 0.9657 - val_loss: 0.0907 - val_acc: 0.9729\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1139 - acc: 0.9665 - val_loss: 0.0900 - val_acc: 0.9735\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1129 - acc: 0.9669 - val_loss: 0.0897 - val_acc: 0.9731\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1133 - acc: 0.9662 - val_loss: 0.0897 - val_acc: 0.9732\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1117 - acc: 0.9660 - val_loss: 0.0898 - val_acc: 0.9730\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1099 - acc: 0.9671 - val_loss: 0.0893 - val_acc: 0.9735\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1097 - acc: 0.9660 - val_loss: 0.0887 - val_acc: 0.9738\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1062 - acc: 0.9672 - val_loss: 0.0891 - val_acc: 0.9731\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1086 - acc: 0.9674 - val_loss: 0.0887 - val_acc: 0.9738\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1087 - acc: 0.9665 - val_loss: 0.0880 - val_acc: 0.9732\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1063 - acc: 0.9676 - val_loss: 0.0889 - val_acc: 0.9738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1070 - acc: 0.9669 - val_loss: 0.0880 - val_acc: 0.9737\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1041 - acc: 0.9689 - val_loss: 0.0885 - val_acc: 0.9731\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1053 - acc: 0.9685 - val_loss: 0.0875 - val_acc: 0.9740\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1045 - acc: 0.9676 - val_loss: 0.0878 - val_acc: 0.9735\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1031 - acc: 0.9700 - val_loss: 0.0880 - val_acc: 0.9738\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1029 - acc: 0.9689 - val_loss: 0.0872 - val_acc: 0.9740\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1035 - acc: 0.9682 - val_loss: 0.0868 - val_acc: 0.9734\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1012 - acc: 0.9697 - val_loss: 0.0870 - val_acc: 0.9735\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1002 - acc: 0.9696 - val_loss: 0.0868 - val_acc: 0.9741\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.1015 - acc: 0.9696 - val_loss: 0.0860 - val_acc: 0.9738\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0993 - acc: 0.9703 - val_loss: 0.0861 - val_acc: 0.9737\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0995 - acc: 0.9701 - val_loss: 0.0857 - val_acc: 0.9743\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0990 - acc: 0.9704 - val_loss: 0.0853 - val_acc: 0.9745\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1001 - acc: 0.9697 - val_loss: 0.0848 - val_acc: 0.9740\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0996 - acc: 0.9697 - val_loss: 0.0853 - val_acc: 0.9743\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0990 - acc: 0.9698 - val_loss: 0.0844 - val_acc: 0.9747\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.1007 - acc: 0.9697 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0963 - acc: 0.9705 - val_loss: 0.0848 - val_acc: 0.9746\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0979 - acc: 0.9702 - val_loss: 0.0843 - val_acc: 0.9741\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0973 - acc: 0.9703 - val_loss: 0.0843 - val_acc: 0.9747\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0977 - acc: 0.9705 - val_loss: 0.0846 - val_acc: 0.9743\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0943 - acc: 0.9715 - val_loss: 0.0840 - val_acc: 0.9745\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0963 - acc: 0.9708 - val_loss: 0.0839 - val_acc: 0.9743\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0934 - acc: 0.9721 - val_loss: 0.0832 - val_acc: 0.9748\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0927 - acc: 0.9718 - val_loss: 0.0841 - val_acc: 0.9746\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0949 - acc: 0.9708 - val_loss: 0.0840 - val_acc: 0.9750\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0952 - acc: 0.9713 - val_loss: 0.0835 - val_acc: 0.9756\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0923 - acc: 0.9715 - val_loss: 0.0840 - val_acc: 0.9753\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0907 - acc: 0.9726 - val_loss: 0.0830 - val_acc: 0.9751\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0949 - acc: 0.9716 - val_loss: 0.0831 - val_acc: 0.9749\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0919 - acc: 0.9720 - val_loss: 0.0830 - val_acc: 0.9755\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0898 - acc: 0.9732 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.0914 - acc: 0.9719 - val_loss: 0.0831 - val_acc: 0.9752\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0920 - acc: 0.9716 - val_loss: 0.0826 - val_acc: 0.9748\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0896 - acc: 0.9735 - val_loss: 0.0820 - val_acc: 0.9756\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0869 - acc: 0.9731 - val_loss: 0.0827 - val_acc: 0.9756\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0913 - acc: 0.9726 - val_loss: 0.0823 - val_acc: 0.9750\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0887 - acc: 0.9733 - val_loss: 0.0820 - val_acc: 0.9758\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0899 - acc: 0.9728 - val_loss: 0.0821 - val_acc: 0.9756\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0870 - acc: 0.9733 - val_loss: 0.0815 - val_acc: 0.9755\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0856 - acc: 0.9744 - val_loss: 0.0819 - val_acc: 0.9758\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0881 - acc: 0.9738 - val_loss: 0.0813 - val_acc: 0.9759\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0863 - acc: 0.9732 - val_loss: 0.0818 - val_acc: 0.9758\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0854 - acc: 0.9742 - val_loss: 0.0814 - val_acc: 0.9759\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0831 - acc: 0.9752 - val_loss: 0.0811 - val_acc: 0.9760\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0852 - acc: 0.9737 - val_loss: 0.0809 - val_acc: 0.9757\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0836 - acc: 0.9745 - val_loss: 0.0806 - val_acc: 0.9756\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0849 - acc: 0.9737 - val_loss: 0.0812 - val_acc: 0.9766\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0840 - acc: 0.9746 - val_loss: 0.0806 - val_acc: 0.9758\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0836 - acc: 0.9739 - val_loss: 0.0805 - val_acc: 0.9759\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0855 - acc: 0.9735 - val_loss: 0.0806 - val_acc: 0.9756\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0836 - acc: 0.9748 - val_loss: 0.0804 - val_acc: 0.9757\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0807 - acc: 0.9748 - val_loss: 0.0799 - val_acc: 0.9760\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0829 - acc: 0.9742 - val_loss: 0.0796 - val_acc: 0.9763\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0816 - acc: 0.9759 - val_loss: 0.0797 - val_acc: 0.9757\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0807 - acc: 0.9758 - val_loss: 0.0796 - val_acc: 0.9763\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0812 - acc: 0.9750 - val_loss: 0.0791 - val_acc: 0.9762\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0811 - acc: 0.9748 - val_loss: 0.0793 - val_acc: 0.9767\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0800 - acc: 0.9755 - val_loss: 0.0787 - val_acc: 0.9768\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0801 - acc: 0.9758 - val_loss: 0.0795 - val_acc: 0.9766\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0803 - acc: 0.9754 - val_loss: 0.0791 - val_acc: 0.9770\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0787 - acc: 0.9755 - val_loss: 0.0790 - val_acc: 0.9768\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0772 - acc: 0.9765 - val_loss: 0.0789 - val_acc: 0.9768\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0779 - acc: 0.9757 - val_loss: 0.0790 - val_acc: 0.9769\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0768 - acc: 0.9765 - val_loss: 0.0790 - val_acc: 0.9763\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0776 - acc: 0.9758 - val_loss: 0.0784 - val_acc: 0.9766\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0794 - acc: 0.9750 - val_loss: 0.0788 - val_acc: 0.9768\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0773 - acc: 0.9762 - val_loss: 0.0787 - val_acc: 0.9763\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0747 - acc: 0.9767 - val_loss: 0.0788 - val_acc: 0.9766\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0796 - acc: 0.9752 - val_loss: 0.0787 - val_acc: 0.9769\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0757 - acc: 0.9763 - val_loss: 0.0794 - val_acc: 0.9763\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.0746 - acc: 0.9771 - val_loss: 0.0787 - val_acc: 0.9769\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0769 - acc: 0.9767 - val_loss: 0.0785 - val_acc: 0.9769\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0762 - acc: 0.9771 - val_loss: 0.0781 - val_acc: 0.9768\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0751 - acc: 0.9773 - val_loss: 0.0787 - val_acc: 0.9770\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0771 - acc: 0.9760 - val_loss: 0.0783 - val_acc: 0.9767\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0748 - acc: 0.9772 - val_loss: 0.0782 - val_acc: 0.9768\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.0755 - acc: 0.9767 - val_loss: 0.0776 - val_acc: 0.9773\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.0736 - acc: 0.9772 - val_loss: 0.0780 - val_acc: 0.9766\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0740 - acc: 0.9767 - val_loss: 0.0775 - val_acc: 0.9770\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0736 - acc: 0.9777 - val_loss: 0.0787 - val_acc: 0.9769\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0732 - acc: 0.9767 - val_loss: 0.0776 - val_acc: 0.9770\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.0722 - acc: 0.9773 - val_loss: 0.0777 - val_acc: 0.9771\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                   batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                   verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 23us/step\n",
      "Test score: 0.07439806119957938\n",
      "Test accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
