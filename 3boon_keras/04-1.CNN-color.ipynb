{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "- model_selection은 모델 최적화에 필요한 다양한 툴 제공\n",
    "- metrics는 모델 결과의 성능 지표 툴\n",
    "- MinMaxScaler는 최댓값 최솟값을 이용해 입력 값 크기 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from keraspp import skeras, sfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(model, nb_classes, in_shape=None):\n",
    "        model.nb_classes = nb_classes\n",
    "        model.in_shape = in_shape\n",
    "        model.build_model() # 모델을 만들고\n",
    "        super().__init__(model.x, model.y) # 부모 초기화 후\n",
    "        model.compile() # 모델 컴파일 (뒤에서 overwrite함)\n",
    "    \n",
    "    def build_model(model):\n",
    "        nb_classes = model.nb_classes\n",
    "        in_shape = model.in_shape\n",
    "        \n",
    "        x = Input(in_shape)\n",
    "        h = Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=in_shape)(x)\n",
    "        h = Conv2D(64, (3, 3), activation='relu')(h)\n",
    "        h = MaxPooling2D(pool_size=(2, 2))(h)\n",
    "        h = Dropout(0.25)(h)\n",
    "        h = Flatten()(h) # conv layer의 처리 결과를 fc layer로 보내기 위한 flatten\n",
    "        z_cl = h # conv layer의 출력 별도 저장\n",
    "        \n",
    "        h = Dense(128, activation='relu')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        z_fl = h # 출력 레이어로 나가기 전 fc layer의 출력 별도 저장\n",
    "        \n",
    "        y = Dense(nb_classes, activation='softmax', name='preds')(h)\n",
    "        model.cl_part = Model(x, z_cl)\n",
    "        model.fl_part = Model(x, z_fl)\n",
    "        \n",
    "        # 해당 모델을 만들 기 위해 입, 출력 멤버변수 초기화\n",
    "        model.x, model.y = x, y\n",
    "    \n",
    "    def compile(model):\n",
    "        Model.compile(model, loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, X, y, nb_classes, scaling=True, test_size=0.2, random_state=0):\n",
    "        self.X = X\n",
    "        self.add_channels()\n",
    "        X = self.X\n",
    "        \n",
    "        # split 함수가 섞어주는 것까지 해준다\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        \n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        \n",
    "        if scaling:\n",
    "            # scaling to have (0, 1) for each feature(pixel)\n",
    "            scaler = MinMaxScaler()\n",
    "            n = X_train.shape[0]\n",
    "            X_train = scaler.fit_transform(X_train.reshape(n, -1)).reshape(X_train.shape)\n",
    "            n = X_test.shape[0]\n",
    "            X_test = scaler.transform(X_test.reshape(n, -1)).reshape(X_test.shape)\n",
    "            self.scaler = scaler\n",
    "            \n",
    "        # one hot encoding\n",
    "        Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "        Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.Y_train, self.Y_test = Y_train, Y_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        \n",
    "    def add_channels(self):\n",
    "        X = self.X\n",
    "        \n",
    "        if len(X.shape) == 3:\n",
    "            N, img_rows, img_cols = X.shape\n",
    "            \n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X = X.reshape(X.shape[0], 1, img_rows, img_cols) # 3으로 해야하는 것 아님? ㄴㄴ 흑백일 경우 들어오는 곳임\n",
    "                input_shape = (1, img_rows, img_cols)\n",
    "            else:\n",
    "                X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "                input_shape = (img_rows, img_cols, 1)\n",
    "        else:\n",
    "            input_shape = X.shape[1:] # channel is already included (개수, row, col, 차원) 가정하는 듯\n",
    "        \n",
    "        self.X = X\n",
    "        self.input_shape = input_shape\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine():\n",
    "    def __init__(self, X, y, nb_classes=2, fig=True):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.set_data(X, y)\n",
    "        self.set_model()\n",
    "        self.fig = fig\n",
    "    \n",
    "    def set_data(self, X, y):\n",
    "        nb_classes = self.nb_classes\n",
    "        self.data = DataSet(X, y, nb_classes)\n",
    "    \n",
    "    def set_model(self):\n",
    "        nb_classes = self.nb_classes\n",
    "        data = self.data\n",
    "        self.model = CNN(nb_classes=nb_classes, in_shape=data.input_shape)\n",
    "    \n",
    "    def fit(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        \n",
    "        history = model.fit(data.X_train, data.Y_train,\n",
    "                           batch_size=batch_size, epochs=nb_epoch,\n",
    "                           verbose=verbose,\n",
    "                           validation_data=(data.X_test, data.Y_test))\n",
    "        return history\n",
    "    \n",
    "    def run(self, nb_epoch=10, batch_size=128, verbose=1):\n",
    "        data = self.data\n",
    "        model = self.model\n",
    "        fig = self.fig\n",
    "        \n",
    "        history = self.fit(nb_epoch=nb_epoch, batch_size=batch_size, verbose=verbose)\n",
    "        score = model.evaluate(data.X_test, data.Y_test, verbose=0)\n",
    "        \n",
    "        print('Confusion Matrix')\n",
    "        Y_test_pred = model.predict(data.X_test, verbose=0)\n",
    "        y_test_pred = np.argmax(Y_test_pred, axis=1)\n",
    "        print(metrics.confusion_matrix(data.y_test, y_test_pred))\n",
    "        \n",
    "        print('Test score:', score[0])\n",
    "        print('Test acc  :', score[1])\n",
    "        \n",
    "        suffix = sfile.unique_filename('datetime')\n",
    "        foldname = 'output_' + suffix\n",
    "        os.makedirs(foldname)\n",
    "        skeras.save_history_history('history_history.npy', history.history, fold=foldname)\n",
    "        model.save_weights(os.path.join(foldname, 'dl_model.h5'))\n",
    "        print('Output result are saved in ', foldname)\n",
    "        \n",
    "        if fig:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            skeras.plot_acc(history)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            skeras.plot_loss(history)\n",
    "            plt.show()\n",
    "        \n",
    "        self.history = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제 코드에선 위에까지를 .py에 넣어놓고 밑에서 임포트해서 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import datasets\n",
    "import keras\n",
    "assert keras.backend.image_data_format() == 'channels_last'\n",
    "\n",
    "class MyMachine(Machine):\n",
    "    def __init__(self):\n",
    "        (X, y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "        super().__init__(X, y, nb_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    m = MyMachine()\n",
    "    m.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jeinsong/anaconda3/envs/env_1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jeinsong/anaconda3/envs/env_1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jeinsong/anaconda3/envs/env_1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 7s 180us/step - loss: 1.8191 - acc: 0.3412 - val_loss: 1.5064 - val_acc: 0.4573\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.4533 - acc: 0.4796 - val_loss: 1.3763 - val_acc: 0.4979\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 1.2971 - acc: 0.5399 - val_loss: 1.1575 - val_acc: 0.5876\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.1954 - acc: 0.5792 - val_loss: 1.1028 - val_acc: 0.6141\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 1.1143 - acc: 0.6099 - val_loss: 1.0616 - val_acc: 0.6206\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.0524 - acc: 0.6302 - val_loss: 1.0363 - val_acc: 0.6315\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 1.0022 - acc: 0.6499 - val_loss: 1.0096 - val_acc: 0.6451\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 0.9541 - acc: 0.6668 - val_loss: 0.9725 - val_acc: 0.6588\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 0.9112 - acc: 0.6796 - val_loss: 0.9456 - val_acc: 0.6639\n",
      "Epoch 10/10\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.6921"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
